# support vector machine

A hyperplane seperates two groups of data points, creating decision boundary. Good for high dimensional spaces. goal is to maximize distance (the margin). 

Support vectors are the data points that lie closest to the decision surface (or hyperplane), and these are most difficult to classify. SVMs maximize the margin around the hyperplane

## incorrect model

```{r message=FALSE, warning=FALSE}

library(Rlab)
library(e1071)
set.seed(0)

n_pts = 500
w = rnorm(n_pts)
z = rnorm(n_pts)
w2 = w^2
x = (-1 + 3*w2 - z)
p = exp(x) / ( 1 + exp(x) )
y = rbern(n_pts,p)
y_factor = as.factor(y)

# Part (1): This is an incorrect model:
# 
m_glm = glm( y_factor ~ w + z, family=binomial )
print( summary(m_glm) )

```
### predictions

```{r}
p_hat = predict( m_glm, newdata=data.frame( w=w, z=z ) )
y_hat = as.double( p_hat > 0.5 )
print( table( y, y_hat ) )
```

## correct model

```{r}
# Part (1): This is the correct model:
#
m_glm = glm( y_factor ~ w2 + z, family=binomial )
print( summary(m_glm) )

```

### predictions

```{r}
p_hat = predict( m_glm, newdata=data.frame( w2=w2, z=z ) )
y_hat = as.double( p_hat > 0.5 )
print( table( y, y_hat ) )

```

### support vector machine

Kernal adds another dimension

```{r}

# SVM trained on two sets of variables
m_svm = svm( y_factor ~ w + z )
y_hat = predict( m_svm, newdata=data.frame( w=w, z=z ) )
print( table( y, y_hat ) )

m_svm = svm( y_factor ~ w2 + z )
y_hat = predict( m_svm, newdata=data.frame( w2=w2, z=z ) )
print( table( y, y_hat ) )

# Linear Kernal
m_svm = svm( y_factor ~ w + z, kernel="linear" )
y_hat = predict( m_svm, newdata=data.frame( w=w, z=z ) )
print( table( y, y_hat ) )

m_svm = svm( y_factor ~ w2 + z, kernel="linear" )
y_hat = predict( m_svm, newdata=data.frame( w2=w2, z=z ) )
print( table( y, y_hat ) )

```

## applied example

train svm

```{r}
library(Rlab)
library(e1071)
library(MASS)
set.seed(0)
Pima.tr = na.omit( Pima.tr )

m_svm = svm( type ~ ., data=Pima.tr )
y_hat = predict( m_svm, newdata=Pima.tr )
print( table( Pima.tr$type, y_hat ) )

```

### linear kernal

```{r}

m_svm = svm( type ~ ., data=Pima.tr, kernel="linear" )
y_hat = predict( m_svm, newdata=Pima.tr )
print( table( Pima.tr$type, y_hat ) )


```
### class.weights

Bias towards the majority class in imbalanced data

```{r}
# Part (2):
#
wts = table(Pima.tr$type)
print( wts / sum(wts) )
wts[1] = 0.34
wts[2] = 0.66

m_svm = svm( type ~ ., data=Pima.tr, class.weights=wts )
y_hat = predict( m_svm, newdata=Pima.tr )
print( table( Pima.tr$type, y_hat ) )

m_svm = svm( type ~ ., data=Pima.tr, class.weights=wts, kernel="linear" )
y_hat = predict( m_svm, newdata=Pima.tr )
print( table( Pima.tr$type, y_hat ) )

```

